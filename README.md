# task2-toy-gpt
This repository contains the deliverables for Internship Task 2 (Day 16–30).

## 📄 Summary Report
- Tokenization methods (BPE, WordPiece, SentencePiece)
- GPT architecture and fundamentals

## 🧠 Toy GPT Model
- File: `tiny_gpt_model.py`
- Minimal transformer-based GPT model built in PyTorch
- Trained on: “To be or not to be.”

- ## 🧪 Tokenizer Comparison Experiment

As part of Task 2, I also completed the hands-on experiment using Hugging Face Tokenizers to compare different strategies (BPE, WordPiece) across GPT-2, BERT, and RoBERTa

- ## Tokenized the same input text using all 3 models
  - Measured:
  - Token count
  - Output tokens
  - Encoding speed
  - 
## 📝 Blog
- File: `toy_gpt_blog_post.md'
