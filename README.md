# task2-toy-gpt
This repository contains the deliverables for Internship Task 2 (Day 16â€“30).

## ğŸ“„ Summary Report
- Tokenization methods (BPE, WordPiece, SentencePiece)
- GPT architecture and fundamentals

## ğŸ§  Toy GPT Model
- File: `tiny_gpt_model.py`
- Minimal transformer-based GPT model built in PyTorch
- Trained on: â€œTo be or not to be.â€

- ## ğŸ§ª Tokenizer Comparison Experiment

As part of Task 2, I also completed the hands-on experiment using Hugging Face Tokenizers to compare different strategies (BPE, WordPiece) across GPT-2, BERT, and RoBERTa

- ## Tokenized the same input text using all 3 models
  - Measured:
  - Token count
  - Output tokens
  - Encoding speed
  - 
## ğŸ“ Blog
- File: `toy_gpt_blog_post.md'
